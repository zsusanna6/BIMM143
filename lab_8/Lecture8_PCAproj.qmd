---
title: "Lecture8_PCA"
author: "Longmei Zhang A17012012"
format: pdf
---

Example:
```{r}
head(mtcars)
```

```{r}

##generates very different values, need to scale it
colMeans(mtcars)
```

```{r}
x <- scale(mtcars)
head(x)
```

```{r}

##means shifted to 0 for all the cars
round(colMeans(x),2)

##STDEV shifted to 1. Scaling makes the data more comparable
```
Keypoint: usually a good idea to scale the data before performing PCA

## Breast Cancer Bioposy Analysis

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
```

```{r}
head(wisc.df)
```

Remove the first `Diagnosis` column from the data frame to delete the "answer". 
```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- wisc.df$diagnosis

```

> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```
There are 569 observations in this dataset

> Q2. How many of the observations have a malignant diagnosis?

```{r}
count_M <- sum(diagnosis == 'M')
## can also use table

table(diagnosis)
```
212 of the observations have a malignant diagnosis

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
sum(endsWith(suffix = '_mean', colnames(wisc.data)))

## grepl() also works

sum(grepl("_mean", colnames(wisc.data)))

```
There are 10 features with the suffix '_mean'. 


# Principal component analysis

```{r}
# Check column means and standard deviations
colMeans(wisc.data)
apply(wisc.data,2,sd)

# the values have significant variations
```

```{r}
#Perform PCA on wisc.data by

wisc.pr <- prcomp(wisc.data, scale. = TRUE)
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

0.4427 of the original variance is captured by PC1


> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs are required to describe 70% of the original variance

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs are required to describe 90% of the original variance


### Interpreting the PCA results



plotting the PCs together: "Main PC score plot", "PC1 vs PC2 plot"

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[, 2], col = as.factor(diagnosis))
```
> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?\

It stands out to me that the malignant samples (represented by red dots) and benign samples are differentiated into separated clusters. There occur to be a clear pattern of distinction between the malignant samples and benign samples. It was a little difficult to understand the meanings of the principal components. But the underlying principle is that PCs incorporate variations within the samples, and that patients in the same clusters have similar characteristics. 


> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[, 3], col = as.factor(diagnosis))
```

A similar pattern is shown in the PC1 vs. PC3 plot. There is a relatively clear separation among the benign samples and the malignant samples. The maglinant samples tend to cluster together, showing similar characteristics. The plot indicates that principal component 1 is capturing a separation of malignant (red) from benign (black) samples. 


```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis ) + 
  geom_point()
```

### Variance explained 

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

### Communicating PCA results

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation['concave.points_mean',1]
```
The loading vector for this feature is -0.2608538

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
summary(wisc.pr)
```

5 Principal components are required to explain 80% of the variance of the data

# Hierarchical clustering

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

At height around 19.5, the clustring model has 4 clusters. 

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)

wisc.hclust <- hclust(data.dist, method="complete")

plot(wisc.hclust)
abline(h = 19.5, col="red", lty=2)
```
###Selecting number of clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 4)
table(wisc.hclust.clusters, diagnosis)
```
```{r}
wisc.hclust.clusters2 <- cutree(wisc.hclust, 6)
table(wisc.hclust.clusters2, diagnosis)
```
> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

No, dividing into 4 clusters generates the best results. When divided into 4 clusters, the majority of the malignant samples are captured by group 1, while the majority of the benign samples are captured by group 3 with a small amount samples in other clusters. 

>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

"ward.D2" generates the best results since it minimizes the sum of squared differences from the center point when merging clusters. In this case, it generates clusters with smallest variances between samples in the same clusters. The tree generated by ward.D2 also looks nicer. 

```{r}
wisc.hclust2 <- hclust(data.dist, method="ward.D2")
plot(wisc.hclust2)
```
# K-mean Clustering

```{r}
wisc.km <- kmeans(scale(wisc.data), centers= 2, nstart= 20)
table(wisc.km$cluster, diagnosis)
```
> Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

In the K-mean model, the majority (343/357) of the benign samples are captured in cluster 2,and the majority (175/212) of the malignant samples are captured in cluster 1. This model separated out the two diagnoses nicely. It generates better results than the hclust results since more samples of different diagnoses were separated into different clusters, and no samples are left outside of the major clusters.  

```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```


# Combine PCA and clustering

our PCA results were in `wisc.pr$x`

```{r}
d <- dist(wisc.pr$x[, 1:3])
hc <- hclust(d, method = "ward.D2")
plot(hc)

##cut the tree into 2 groups

abline(h=65, col="red", lty=2)
grps <- cutree(hc, k=2)

```

```{r}

plot(wisc.pr$x, col=grps)


```

compare my clustering result (my 'grps') to the experts `diagnosis`
```{r}
table(diagnosis, grps)
```


> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

In the newly created model, the majority (333/357) of the benign samples are captured in cluster 2,and the majority (179/212) of the malignant samples are captured in cluster 1. This model separated out the two diagnoses nicely. 



> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```

Results from the PCA model is similar with the results from k-mean model. The PCA model has better results than the hierarchical clustering model since more samples of different diagnoses were separated into different clusters, and no samples are left outside of the major clusters.

# Sensitivity/Specificity

> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

Sensitivity: 
PCA: 179/212  K-mean: 175/212  Hierarchical clustering: 165/212

Specificity: 
PCA: 333/366=0.910  K-mean: 343/380=0.903  Hierarchical clustering: 343/383=0.900

PCA has the best sensitivity and specificity, k-mean has the second best sensitivity and specificity. Hierarchical is ranked last among the three methods. 

# Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
head(npc)
```

```{r}

g <- as.factor(grps)
g <- relevel(g,2)

plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q18. Which of these new patients should we prioritize for follow up based on your results?

Group 2 patients should be prioritized. 